# -*- coding: utf-8 -*-
"""DL Project 6 - CIFAR 10 - Object Recognition using ResNet50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eBsE0IRM5w6sJPM50GS7JMnzyHDnqjsI

1.  To classify 10 types of images (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck) from 60 000 images with 32x32 pixels - Classification Problem
2.  Deep Learning Model - Pre-Trained ResNest50 Model
3.  Work Flow

    *   Collect Images Data - Kaggle Dataset (API)
    *   Data Pre-Processing
    *   Train-Test Split
    *   Deep Learning Model - Pre-Trained ResNest50 Model
    *   DL Model Evaluation
    *   Develop Prediction System - Feed new data to trained model to predict image of either an airplane, automobile, bird, cat, deer, dog, frog, horse, ship or truck.
"""

# configuring the path of kaggle.json file

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# kaggle api

!kaggle competitions download -c cifar-10

!ls

# extracting the compressed file - cifar-10 zip file

from zipfile import ZipFile

dataset = '/content/cifar-10.zip'

with ZipFile(dataset, 'r') as zip:
  zip.extractall()
  print('The dataset is extracted')

!ls

# since the test and train file are in 7z file (not a zip file), we have to install py7zr library to extract the files

!pip install py7zr

import py7zr

archive = py7zr.SevenZipFile('/content/train.7z', mode='r')
archive.extractall()
archive.close()

!ls

"""Importing the Dependencies"""

import os
import numpy as np
import pandas as pd
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.model_selection import train_test_split

filenames = os.listdir('/content/train')

type(filenames)

len(filenames)

print(filenames[0:5])
print(filenames[-5:])

"""Labels Processing"""

labels_df = pd.read_csv('/content/trainLabels.csv')

labels_df.shape

labels_df.head()

labels_df[labels_df['id'] == 7769]

labels_df.head(10)

labels_df.tail(10)

labels_df['label'].value_counts()

labels_df['label']

# label encoding - changing text label to numerical label

labels_dictionary = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}

labels = [labels_dictionary[i] for i in labels_df['label']]

print(labels[0:5])
print(labels[-5:])

# displaying sample images

import cv2
from google.colab.patches import cv2_imshow

img = cv2.imread('/content/train/1.png')
cv2_imshow(img)

id_list = list(labels_df['id'])

print(id_list[0:5])
print(id_list[-5:])

"""Image Processing"""

# convert images to numpy arrays

train_data_folder = '/content/train/'

data = []

for id in id_list:
  image = Image.open(train_data_folder + str(id) + '.png')
  image = np.array(image)
  data.append(image)

type(data)

len(data)

type(data[0])

data[0].shape

data[0]

# convert images list and labels list to numpy arrays
# changing data type (list --> numpy array)

X = np.array(data)
Y = np.array(labels)

type(X)

print(X.shape)
print(Y.shape)

"""Train-Test Split"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

"""Standardisation / Normalization"""

# scaling the data - to increase the NN accuracy

X_train_scaled = X_train/255
X_test_scaled = X_test/255

X_train_scaled[0]

"""Deep Learning Training - Neural Network (NN)"""

import tensorflow as tf # tensorflow in a base library
from tensorflow import keras # keras is a easier library within tensorflow

num_of_classes = 10

# setting up layers of Neural Network (NN)

model = keras.Sequential([

                          keras.layers.Flatten(input_shape=(32,32,3)),             # input layer
                          keras.layers.Dense(64, activation='relu'),               # hidden layer
                          keras.layers.Dense(num_of_classes, activation='softmax') # output layer
                                                             # softmax - suitable for multiple clasess
])

# compile the neural network (NN)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])

# training the neural network (NN)

model.fit(X_train_scaled, Y_train, validation_split=0.1, epochs=10)

"""ResNet50"""

from tensorflow.keras import Sequential, models, layers
from tensorflow.keras import optimizers
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import load_model
from tensorflow.keras.models import Model
from tensorflow.keras.applications.resnet50 import ResNet50

convolutional_base = ResNet50(weights='imagenet', include_top=False, input_shape=(256,256,3))
                              # imagenet - weight for very large image data (14++ millions data)
                              # include_top=False - not include the output layer
convolutional_base.summary()

num_of_classes = 10

model = models.Sequential()

# scaling image from 32x32 to 256x256 by multiplying the height and width by 2
model.add(layers.UpSampling2D((2,2))) # (32x32)*2 = 64x64
model.add(layers.UpSampling2D((2,2))) # (64x64)*2 = 128x128
model.add(layers.UpSampling2D((2,2))) # (128x128)*2 = 256x256

# passing the scaling to convolutional_base model
model.add(convolutional_base)

# input layer
# flatten the matrices to a single vector
# normalized the output data from input layer into a same range to be sent to the next layer
model.add(layers.Flatten())
model.add(layers.BatchNormalization())

# hidden layer
# first layer - 128 neurons with relu activation function
# second layer - 64 neurons with relu activation function
# Dropout(0.5) - to reduce the risk of overfitting - turn off several neurons by giving the value 0
# normalized the output data from first hidden layer into a same range to be sent to the second hidden layer
# normalized the output data from second hidden layer into a same range to be sent to the output layer
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.BatchNormalization())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.BatchNormalization())

# output layer
# softmax activation function is more suitable for multiple classification problems
model.add(layers.Dense(num_of_classes, activation='softmax'))

model.compile(optimizer=optimizers.RMSprop(learning_rate=2e-5),
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])

history = model.fit(X_train_scaled, Y_train, validation_split=0.1, epochs=10)

loss, accuracy = model.evaluate(X_test_scaled, Y_test)
print('Test accuracy = ', accuracy)

h = history

# plot the loss value
plt.plot(h.history['loss'], label='train loss')
plt.plot(h.history['val_loss'], label='validation loss')
plt.legend()
plt.show()

# plot the accuracy value
plt.plot(h.history['acc'], label='train accuracy')
plt.plot(h.history['val_acc'], label='validation accuracy')
plt.legend()
plt.show()