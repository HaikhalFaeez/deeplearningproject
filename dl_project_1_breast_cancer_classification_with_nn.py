# -*- coding: utf-8 -*-
"""DL Project 1 - Breast Cancer Classification with NN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S9WmsMAc6nEjX2EBcQaTaOqnTuKa81fW

1.  To classify the breast cancer either the cancer is benign tumor or malignant tumor - Binary Classification Problem
2.  Deep Learning Model - Neural Network (NN)
3.  Work Flow

    *   Collect Fine Needle Aspiration Data (A Type of Biopsy Procedure) - Scikit-learn Dataset
    *   Data Pre-Processing
    *   Train-Test Split
    *   Deep Learning Model - Neural Network Model (DL)
    *   DL Model Evaluation
    *   Develop Prediction System - Feed new data to trained model to predict the breast cancer condition

Import the Dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.datasets
from sklearn.model_selection import train_test_split

"""Data Collection & Pre-Processing"""

cancer_data = sklearn.datasets.load_breast_cancer()

print(cancer_data)

data_frame = pd.DataFrame(cancer_data.data, columns = cancer_data.feature_names)

data_frame.head()

data_frame['label'] = cancer_data.target

data_frame.tail()

data_frame.shape

data_frame.info()

data_frame.isnull().sum()

data_frame.describe()

data_frame['label'].value_counts()

"""1 --> Benign

0 --> Malignant
"""

data_frame.groupby('label').mean()

X = data_frame.drop(columns = 'label', axis = 1)
Y = data_frame['label']

print(X)

print(Y)

"""Train-Test Split"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 2)

print(X.shape, X_train.shape, X_test.shape)

"""Data Standardisation"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_std = scaler.fit_transform(X_train)

X_test_std = scaler.transform(X_test)

"""Deep Learning Training - Neural Network (NN)"""

# importing Tensorflow and Keras

import tensorflow as tf #develop by Google
tf.random.set_seed(3)   #somewhat similar to random_state
from tensorflow import keras

# setting up the layers in NN

model = keras.Sequential([
                          keras.layers.Flatten(input_shape=(30,)),     # input layer # number of neurons = number of features / columns # Flatten - to convert the data into a single dimensional array
                          keras.layers.Dense(20, activation='relu'),   # hidden layers
                          keras.layers.Dense(2, activation='sigmoid')  # output layer # number of neurons = number of class (0 & 1) = in this case, 2 class only
])

# compiling the neural network

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# training the neural network

history = model.fit(X_train_std, Y_train, validation_split=0.1, epochs=10)

"""Visualizing accuracy and loss"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')

plt.legend(['training data', 'validation data'], loc = 'lower right')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')

plt.legend(['training data', 'validation data'], loc = 'upper right')

"""Accuracy of the Model on Test Data"""

loss, accuracy = model.evaluate(X_test_std, Y_test)
print(accuracy)

print(X_test_std.shape)
print(X_test_std[0])

Y_pred = model.predict(X_test_std)

print(Y_pred.shape)
print(Y_pred[0])

# instead of outcome of 1 or 0, the result show 2 float value
# [0.2413, 0.5460] = probability of the label is [0, 1]
# meaning that the probability of the data to be labeled as 0 is 0.2413 while the probability of the data to be labeled as 1 is 0.5460
# so the maximum is the 0.5460 meaning that the prediction is likely to be labeled as 1 (Malignant)

print(X_test_std)

print(Y_pred)

# argmax function
# give the index value for the maximum value in the list

my_list = [10, 20, 30]

index_of_max_value = np.argmax(my_list)
print(my_list)
print(index_of_max_value)

# converting the prediction probability to class label (either 0 or 1)

Y_pred_labels = [np.argmax(i) for i in Y_pred]
print(Y_pred_labels)

"""Building the Predictive System"""

input_data = (13.08,15.71,85.63,520,0.1075,0.127,0.04568,0.0311,0.1967,0.06811,0.1852,0.7477,1.383,14.67,0.004097,0.01898,0.01698,0.00649,0.01678,0.002425,14.5,20.49,96.09,630.5,0.1312,0.2776,0.189,0.07283,0.3184,0.08183)

# change input data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the numpy array as a single data point
input_data_reshaped = input_data_as_numpy_array.reshape(1, -1)

# standardising the reshaped input data
input_data_std = scaler.transform(input_data_reshaped)

prediction = model.predict(input_data_std)
print(prediction)

prediction_label = [np.argmax(prediction)]
print(prediction_label)

if (prediction_label[0] == 0):
  print('The tumor is malignant')

else:
  print('The tumor is benign')